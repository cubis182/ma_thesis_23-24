p.values[i] <- kstest.on.current.jittered.data$p.value   # store the current jittering's p
} # end of for-loop: do the following 1000 times
summary(D.values) # explore the distribution of the D-values
summary(p.values) # explore the distribution of the p-values
test.one <- zeta.table$SENTLEN[zeta.table$WORK %in% poetry]
test.two <- zeta.table$SENTLEN[zeta.table$WORK == "Sati"]
D.values <- p.values <- rep(NA, 1000) # reserve two collector vectors for 1000 D- & p-values
set.seed(sum(utf8ToInt("Plörkel")))   # set a replicable random number seed
for (i in 1:1000) { # do the following 1000 times
kstest.on.current.jittered.data <- # store the result of a
ks.test(exact=FALSE,            # K-S test (but not the exact one) of
x=jitter(test.one), # colloquial position
y=jitter(test.two),
alternative="t")   # non-colloquial position
D.values[i] <- kstest.on.current.jittered.data$statistic # store the current jittering's D
p.values[i] <- kstest.on.current.jittered.data$p.value   # store the current jittering's p
} # end of for-loop: do the following 1000 times
summary(D.values) # explore the distribution of the D-values
summary(p.values) # explore the distribution of the p-values
#pronouns
barplot(height=c(sum(nu.table$PRONOUN[nu.table$WORK == "Carm"])/sum(nu.table$SENTLEN[nu.table$WORK == "Carm"])*10000, sum(nu.table$PRONOUN[nu.table$WORK == "Sati"])/sum(nu.table$SENTLEN[nu.table$WORK == "Sati"])*10000, sum(nu.table$PRONOUN[nu.table$WORK == "Fab"])/sum(nu.table$SENTLEN[nu.table$WORK == "Fab"])*10000, sum(nu.table$PRONOUN[nu.table$WORK %in% poetry])/sum(nu.table$SENTLEN[nu.table$WORK %in% poetry])*10000, sum(nu.table$PRONOUN[nu.table$WORK %in% colloquial.nosat])/sum(nu.table$SENTLEN[nu.table$WORK %in% colloquial.nosat])*10000), col = c("red", "yellow", "brown", "green", "blue"), names.arg= c("Catullus", "Juvenal", "Fables", "All Poetry", "Colloquial"), main="Normed frequency of pronouns in a selection of texts", ylab="Frequency per 10,000")
#subordinate clauses
barplot(height=c(sum(zeta.table$SUB[zeta.table$WORK == "Carm"])/sum(zeta.table$SENTLEN[zeta.table$WORK == "Carm"])*10000, sum(zeta.table$SUB[zeta.table$WORK == "Sati"])/sum(zeta.table$SENTLEN[zeta.table$WORK == "Sati"])*10000, sum(zeta.table$SUB[zeta.table$WORK == "Fab"])/sum(zeta.table$SENTLEN[zeta.table$WORK == "Fab"])*10000, sum(zeta.table$SUB[zeta.table$WORK %in% poetry])/sum(zeta.table$SENTLEN[zeta.table$WORK %in% poetry])*10000, sum(zeta.table$SUB[zeta.table$WORK %in% colloquial.nosat])/sum(zeta.table$SENTLEN[zeta.table$WORK %in% colloquial.nosat])*10000, sum(zeta.table$SUB[zeta.table$WORK == "Gall"])/sum(zeta.table$SENTLEN[zeta.table$WORK %in% "Gall"])*10000), col = c("red", "yellow", "brown", "green", "blue", "tan"), names.arg= c("Catullus", "Juvenal", "Fables", "All Poetry", "Colloquial", "Caesar"), main=paste(strwrap("Normed frequency of subordinate clauses in a selection of texts", width=50), collapse="\n"), ylab="Frequency per 10,000")
test.one <- zeta.table$SUB[zeta.table$WORK %in% colloquial.nosat & zeta.table$SUB > 0]
test.two <- zeta.table$SUB[zeta.table$WORK %in% colloquial.nosat == FALSE & zeta.table$SUB > 0]
D.values <- p.values <- rep(NA, 1000) # reserve two collector vectors for 1000 D- & p-values
set.seed(sum(utf8ToInt("Plörkel")))   # set a replicable random number seed
for (i in 1:1000) { # do the following 1000 times
kstest.on.current.jittered.data <- # store the result of a
ks.test(exact=FALSE,            # K-S test (but not the exact one) of
x=jitter(test.one), # colloquial position
y=jitter(test.two),
alternative="t")   # non-colloquial position
D.values[i] <- kstest.on.current.jittered.data$statistic # store the current jittering's D
p.values[i] <- kstest.on.current.jittered.data$p.value   # store the current jittering's p
} # end of for-loop: do the following 1000 times
summary(D.values) # explore the distribution of the D-values
summary(p.values) # explore the distribution of the p-values
test.two <- zeta.table$SUB[(zeta.table$WORK %in% colloquial.nosat) == FALSE & zeta.table$SUB > 0]
D.values <- p.values <- rep(NA, 1000) # reserve two collector vectors for 1000 D- & p-values
set.seed(sum(utf8ToInt("Plörkel")))   # set a replicable random number seed
for (i in 1:1000) { # do the following 1000 times
kstest.on.current.jittered.data <- # store the result of a
ks.test(exact=FALSE,            # K-S test (but not the exact one) of
x=jitter(test.one), # colloquial position
y=jitter(test.two),
alternative="t")   # non-colloquial position
D.values[i] <- kstest.on.current.jittered.data$statistic # store the current jittering's D
p.values[i] <- kstest.on.current.jittered.data$p.value   # store the current jittering's p
} # end of for-loop: do the following 1000 times
summary(D.values) # explore the distribution of the D-values
summary(p.values) # explore the distribution of the p-values
test.two <- zeta.table$SUB[(zeta.table$WORK %in% prose.noncoll) & zeta.table$SUB > 0]
D.values <- p.values <- rep(NA, 1000) # reserve two collector vectors for 1000 D- & p-values
set.seed(sum(utf8ToInt("Plörkel")))   # set a replicable random number seed
for (i in 1:1000) { # do the following 1000 times
kstest.on.current.jittered.data <- # store the result of a
ks.test(exact=FALSE,            # K-S test (but not the exact one) of
x=jitter(test.one), # colloquial position
y=jitter(test.two),
alternative="t")   # non-colloquial position
D.values[i] <- kstest.on.current.jittered.data$statistic # store the current jittering's D
p.values[i] <- kstest.on.current.jittered.data$p.value   # store the current jittering's p
} # end of for-loop: do the following 1000 times
summary(D.values) # explore the distribution of the D-values
summary(p.values) # explore the distribution of the p-values
sort(proportions(table(eta.table$LEM[eta.table$WORK %in% poetry & eta.table$TYPEB == "hypo"])), decreasing=TRUE)[1:55] * 100
sort(proportions(table(eta.table$LEM[eta.table$WORK %in% prose.noncoll & eta.table$TYPEB == "hypo"])), decreasing=TRUE)[1:55] * 100
sort(proportions(table(eta.table$LEM[eta.table$TYPEB == "hypo"])), decreasing=TRUE)[1:55] * 100
##########CLAUSE DETAIL####################
iota.table <- read.csv("C:/Users/T470s/Documents/GitHub/ma_thesis_23-24/Data-output/var-info/all-clause-3.16.24.csv", stringsAsFactors = TRUE, comment.char = "#")
sort(proportions(table(iota.table$LEM[iota.table$WORK %in% poetry])), decreasing=TRUE)[1:55] * 100
head(iota.table)
sort(proportions(table(iota.table$SUB[iota.table$WORK %in% poetry])), decreasing=TRUE)[1:55] * 100
View(eta.table)
sort(proportions(table(eta.table$LEM[eta.table$WORK %in% prose.noncoll & eta.table$TYPEB == "para"])), decreasing=TRUE)[1:55] * 100
summary(eta.table)
sort(proportions(table(eta.table$LEM[eta.table$WORK %in% prose.noncoll & eta.table$TYPEB == "para" & eta.table$TYPEA %in% c("temporal", "causal", "spatial", "mixed")])), decreasing=TRUE)[1:55] * 100
sort(proportions(table(eta.table$LEM[eta.table$WORK %in% poetry & eta.table$TYPEB == "para" & eta.table$TYPEA %in% c("temporal", "causal", "spatial", "mixed")])), decreasing=TRUE)[1:55] * 100
sort(proportions(table(eta.table$LEM[eta.table$WORK %in% colloquial.nosat & eta.table$TYPEB == "para" & eta.table$TYPEA %in% c("temporal", "causal", "spatial", "mixed")])), decreasing=TRUE)[1:55] * 100
sort(proportions(table(eta.table$LEM[eta.table$WORK %in% c("Met", "Aen") & eta.table$TYPEB == "para" & eta.table$TYPEA %in% c("temporal", "causal", "spatial", "mixed")])), decreasing=TRUE)[1:55] * 100
sort(proportions(table(eta.table$LEM[eta.table$TYPEB == "para" & eta.table$TYPEA %in% c("temporal", "causal", "spatial", "mixed")])), decreasing=TRUE)[1:55] * 100
table(mu.table$SENT.ADDR[mu.table$PARENTH == "id est"])
sort(table(mu.table$SENT.ADDR[mu.table$PARENTH == "id est"]), decreasing = TRUE)[1]
sort(table(mu.table$SENT.ADDR[mu.table$PARENTH == "id est"]), decreasing = FALSE)[1]
mu.table$SENT.ADDR[mu.table$PARENTH == "id est"]
sort(table(mu.table$SENT.ADDR[mu.table$FULL.PARENTH == "id est"]), decreasing = FALSE)[1]
mu.table[mu.table$FULL.PARENTH == "id est",]
mu.table$SENT.ADDR[mu.table$FULL.PARENTH == "id est"]
sort(table(mu.table$SENT.ADDR[mu.table$FULL.PARENTH == "id est"]))
sort(table(mu.table$SENT.ADDR[mu.table$FULL.PARENTH == "id est"]), decreasing = FALSE)
sort(table(mu.table$SENT.ADDR[mu.table$FULL.PARENTH == "id est"]), decreasing = TRUE)
sort(table(mu.table$SENT.ADDR[mu.table$FULL.PARENTH == "id est"]), decreasing = TRUE)[1]
max(zeta.table$PRED[zeta.table$WORK == "Pere"])
zeta.table$SENT.ADDR[zeta.table$WORK == "Pere" & zeta.table$PRED == 8]
sort(table(mu.table$SENT.ADDR[mu.table$FULL.PARENTH == "id est"]), decreasing = FALSE)
mu.table[mu.table$FULL.PARENTH == "id est" & mu.table$WORK != "Pere"]
mu.table[mu.table$FULL.PARENTH == "id est" & mu.table$WORK != "Pere",]
mu.table[mu.table$FULL.PARENTH == "hoc est",]
mu.table[mu.table$FULL.PARENTH == "hoc est ",]
max(zeta.table$PRED[zeta.table$WORK == "Att"])
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 8]
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 7]
View(before)
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 6]
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 3]
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$SENTLEN < 8]
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 6]
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 8]
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 9]
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 6]
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 5]
zeta.table$SENT.ADDR[zeta.table$WORK == "Att" & zeta.table$PRED == 4]
max(zeta.table$SENT.ADDR[zeta.table$WORK %in% "Petr Speech"])
max(zeta.table$SENTLEN[zeta.table$WORK %in% "Petr Speech"])
?order
?rbind
zeta.table[order(zeta.table$PRED),]
zeta.table[order(zeta.table$PRED, decreasing = TRUE),]
max(zeta.table$SENTLEN[zeta.table$WORK %in% "Petr Speech"])
zeta.table[order(SENTLEN),]
zeta.table[order(zeta.table$SENTLEN),]
zeta.table[order(zeta.table$SENTLEN, decreasing = TRUE),]
petr <- zeta.table[zeta.table$WORK == "Petr Speech",]
petr[order(petr$SENTLEN, decreasing = TRUE),]
petr[order(petr$PRED, decreasing = TRUE),]
order(petr$PRED, decreasing = TRUE)
sort(petr$PRED, decreasing = TRUE)
petr$SENT.ADDR[sort(petr$PRED, decreasing = TRUE)]
(petr$SENT.ADDR[sort(petr$PRED, decreasing = TRUE)])[1]
(petr$SENT.ADDR[sort(petr$PRED, decreasing = TRUE)])[2]
(petr$PRED[sort(petr$PRED, decreasing = TRUE)])[1]
petr$PRED[sort(petr$PRED, decreasing = TRUE)]
petr[petr$PRED == 0,]
(petr$PRED[sort(petr$PRED, decreasing = TRUE)])[1]
(petr$PRED[sort(petr$PRED, decreasing = TRUE)])[1]
(petr$SENT.ADDR[sort(petr$PRED, decreasing = TRUE)])[1]
?pie
?pie
?apply
apply(zeta.table$PRED, length)
apply(zeta.table$PRED, FUN=length)
tapply(X=zeta.table$PRED, INDEX=zeta.table$PRED, FUN=length)
as.vector(tapply(X=zeta.table$PRED, INDEX=zeta.table$PRED, FUN=length))
as.vector(tapply(X=zeta.table$PRED, INDEX=zeta.table$PRED, FUN=length))[2:4]
#overall:
overall.preds <- as.vector(tapply(X=zeta.table$PRED, INDEX=zeta.table$PRED, FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(overall.preds)
coll.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% colloquial.nosat], INDEX=zeta.table$PRED[zeta.table$WORK %in% colloquial.nosat], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(coll.preds)
prose.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% prose.noncoll], INDEX=zeta.table$PRED[zeta.table$WORK %in% prose.noncoll], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(prose.preds)
prose.preds
coll.preds
pie(prose.preds)
poet.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% poetry], INDEX=zeta.table$PRED[zeta.table$WORK %in% poetry], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(poet.preds)
poet.preds
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Petr Speech"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Petr Speech"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Att"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Att"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Fab"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Att"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Fab"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Fab"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Petr Speech"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Petr Speech"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Fab"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Fab"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Cael"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Fab"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Cael"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Cael"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Gall"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Gall"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Gall"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Gall"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Petr Speech"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Petr Speech"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Pere"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Gall"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Pere"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Pere"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Petr Speech"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Petr Speech"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "agri"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "agri"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Met"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Met"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "Sati"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "Sati"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
petr.preds <- as.vector(tapply(X=zeta.table$PRED[zeta.table$WORK %in% "off"], INDEX=zeta.table$PRED[zeta.table$WORK %in% "off"], FUN=length))[2:4] #find out the number of preds in each sentence; 2:4 goes from sentences with 1, 2 and 3 predicates
pie(petr.preds)
#barplot
barplot(c(sum(nu.table$PARTICIPLE[nu.table$WORK %in% colloquial.nosat & nu.table$WORK != 'Pere'])/sum(nu.table$SENTLEN[nu.table$WORK %in% colloquial.nosat & nu.table$WORK != "Pere"])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% poetry])/sum(nu.table$SENTLEN[nu.table$WORK %in% poetry])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% prose.noncoll])/sum(nu.table$SENTLEN[nu.table$WORK %in% prose.noncoll])*10000), names=c("Colloquial (no Egeria)", "Poetry", "Prose"), col = c("blue", "green", "black"), ylab = "Occurrence per 10,000 words", main = "Normed frequency of participles", ylim=c(0,2000))
#barplot
barplot(c(sum(nu.table$PARTICIPLE[nu.table$WORK %in% colloquial.nosat & nu.table$WORK != 'Pere'])/sum(nu.table$SENTLEN[nu.table$WORK %in% colloquial.nosat & nu.table$WORK != "Pere"])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% poetry])/sum(nu.table$SENTLEN[nu.table$WORK %in% poetry])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% prose.noncoll])/sum(nu.table$SENTLEN[nu.table$WORK %in% prose.noncoll])*10000), names=c("Colloquial (no Egeria", "Poetry", "Prose"), col = c("blue", "green", "black"), ylab = "Occurrence per 10,000 words", main = "Normed frequency of participles", ylim=c(0,2000))
#barplot
barplot(c(sum(nu.table$PARTICIPLE[nu.table$WORK %in% colloquial.nosat & nu.table$WORK != 'Pere'])/sum(nu.table$SENTLEN[nu.table$WORK %in% colloquial.nosat & nu.table$WORK != "Pere"])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% poetry])/sum(nu.table$SENTLEN[nu.table$WORK %in% poetry])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% prose.noncoll])/sum(nu.table$SENTLEN[nu.table$WORK %in% prose.noncoll])*10000), names=c("Colloquial (no Egeria)", "Poetry", "Prose"), col = c("blue", "green", "black"), ylab = "Occurrence per 10,000 words", main = "Normed frequency of participles", ylim=c(0,2000))
barplot(c(sum(nu.table$PARTICIPLE[nu.table$WORK %in% colloquial.nosat])/sum(nu.table$SENTLEN[nu.table$WORK %in% colloquial.nosat])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% poetry])/sum(nu.table$SENTLEN[nu.table$WORK %in% poetry])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% prose.noncoll])/sum(nu.table$SENTLEN[nu.table$WORK %in% prose.noncoll])*10000), names=c("Colloquial", "Poetry", "Prose"), col = c("blue", "green", "black"), ylab = "Occurrence per 10,000 words", main = "Normed frequency of participles", ylim=c(0,2000))
barplot(c(sum(nu.table$PARTICIPLE[nu.table$WORK %in% colloquial.nosat])/sum(nu.table$SENTLEN[nu.table$WORK %in% colloquial.nosat])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% poetry])/sum(nu.table$SENTLEN[nu.table$WORK %in% poetry])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% prose.noncoll])/sum(nu.table$SENTLEN[nu.table$WORK %in% prose.noncoll])*10000), names=c("Colloquial", "Poetry", "Prose"), col = c("blue", "green", "black"), ylab = "Occurrence per 10,000 words", main = "Normed frequency of participles", ylim=c(0,2000))
#noun phrase barplot
barplot(c(sum(nu.table$NOUN[nu.table$WORK %in% colloquial.nosat])/sum(nu.table$SENTLEN[nu.table$WORK %in% colloquial.nosat]), sum(nu.table$NOUN[nu.table$WORK %in% poetry])/sum(nu.table$SENTLEN[nu.table$WORK %in% poetry]), sum(nu.table$NOUN[nu.table$WORK %in% prose.noncoll])/sum(nu.table$SENTLEN[nu.table$WORK %in% prose.noncoll])), names=c("Colloquial", "Poetry", "Prose"), col = c("blue", "green", "black"), ylab = "Percentage", main = "Percentage of words in noun phrases", ylim=c(0,1))
#barplot
barplot(c(sum(nu.table$PARTICIPLE[nu.table$WORK %in% colloquial.nosat & nu.table$WORK != 'Pere'])/sum(nu.table$SENTLEN[nu.table$WORK %in% colloquial.nosat & nu.table$WORK != "Pere"])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% poetry])/sum(nu.table$SENTLEN[nu.table$WORK %in% poetry])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% prose.noncoll])/sum(nu.table$SENTLEN[nu.table$WORK %in% prose.noncoll])*10000), names=c("Colloquial (no Egeria)", "Poetry", "Prose"), col = c("blue", "green", "black"), ylab = "Occurrence per 10,000 words", main = "Normed frequency of participles", ylim=c(0,2000))
barplot(c(sum(nu.table$PARTICIPLE[nu.table$WORK %in% colloquial.nosat])/sum(nu.table$SENTLEN[nu.table$WORK %in% colloquial.nosat])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% poetry])/sum(nu.table$SENTLEN[nu.table$WORK %in% poetry])*10000, sum(nu.table$PARTICIPLE[nu.table$WORK %in% prose.noncoll])/sum(nu.table$SENTLEN[nu.table$WORK %in% prose.noncoll])*10000), names=c("Colloquial", "Poetry", "Prose"), col = c("blue", "green", "black"), ylab = "Occurrence per 10,000 words", main = "Normed frequency of participles", ylim=c(0,2000))
#Notes on the following ecdf plot: poetry and prose are quite close in distribution of words within participle phrases, while the colloquial curve is less than the other two by quite a bit: DO THE SIGNIFICANCE TESTS. If we only look at sentences where there is one or more participle
ggplot(nu.table[nu.table$WORK %in% poetry,], aes(PARTICIPLE)) + stat_ecdf(geom="point", colour = "green") + stat_ecdf(data=nu.table[nu.table$WORK %in% colloquial.nosat,],aes(PARTICIPLE), geom="point", colour = "blue") + stat_ecdf(data=nu.table[nu.table$WORK %in% prose.noncoll,], mapping=aes(PARTICIPLE), geom="point", colour = "black") + labs(title="Ecdf of number of words in participial phrases in each sentence in colloquial, poetic, and prosaic texts.") + scale_x_continuous(n.breaks = 20, limits = c(0,25)) + scale_y_continuous(n.breaks = 10)
#participles
cortests <- c(prose.noncoll, poetry)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=PARTICIPLE, y=SENTLEN)) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm")
library(ggplot2)
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=PARTICIPLE, y=SENTLEN)) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm")
#participles
cortests <- c(colloquial.nosat)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=PARTICIPLE, y=SENTLEN)) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm")
?jitter
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm")
?ggplot
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
#prepositions
cortests <- c(colloquial.nosat, prose.noncoll)
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75]), method="spearman")
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40 & nu.table$SENTLEN < 75]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75 & nu.table$PARTICIPLE < 40]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 20)) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + labs(title=paste("Participles in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
#participles
par(mfrow=c(1,2))
cortests <- c(colloquial.nosat)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + labs(title=paste("Participles in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40 & nu.table$SENTLEN < 75]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75 & nu.table$PARTICIPLE < 40]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(colloquial.nosat)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + labs(title=paste("Participles in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40) + ylim(c(0,100)) + labs(title=paste("Participles in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40 & nu.table$SENTLEN < 75]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75 & nu.table$PARTICIPLE < 40]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40) + ylim(c(0,100))) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Participles in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40 & nu.table$SENTLEN < 75]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75 & nu.table$PARTICIPLE < 40]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40) + ylim(c(0,100))) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40 & nu.table$SENTLEN < 75]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75 & nu.table$PARTICIPLE < 40]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40 & nu.table$SENTLEN < 75]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75 & nu.table$PARTICIPLE < 40]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
#prepositions
cortests <- c(colloquial.nosat, prose.noncoll)
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40 & nu.table$SENTLEN < 75]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75 & nu.table$PARTICIPLE < 40]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(colloquial.nosat)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Participles in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40 & nu.table$SENTLEN < 75]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75 & nu.table$PARTICIPLE < 40]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Participles in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length")
cortests <- c(colloquial.nosat)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$SUB[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length")
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$SUB[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length")
cortests <- c(colloquial.nosat)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$SUB[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length")
cortests <- c(poetry, prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$SUB[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length")
View(nu.table)
View(nu.table[nu.table$WORK %in% colloquial.nosat])
View(nu.table[nu.table$WORK %in% colloquial.nosat,])
cortests <- c(colloquial.nosat)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Participles in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length")
cortests <- c("Att", "Petr Speech", "Fab")
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$SUB[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length") + wrap_labs()
library(ggplot2)
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length") + wrap_labs()
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length")
mean(zeta.table$SUB[zeta.table$WORK == "Petr Speech"])
mean(zeta.table$SUB[zeta.table$WORK == "Gall"])
mean(zeta.table$SUB[zeta.table$WORK == "Fab"])
cortests <- c("Petr Speech", "Fab")
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$SUB[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length")
cortests <- c(poetry)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$SUB[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in prose and poetry vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length")
cortests <- c("Att")
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$SUB[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(SUB, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Subordinate clauses in colloquial texts vs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of subordinate clauses") + ylab("Sentence length")
spearman
cortests <- c(prose.noncoll)
set.seed(sum(utf8ToInt("stuffandthings")))
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests & nu.table$PARTICIPLE < 40 & nu.table$SENTLEN < 75]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests & nu.table$SENTLEN < 75 & nu.table$PARTICIPLE < 40]), method="spearman")
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 40)) + ylim(c(0,100)) + labs(title=paste("Participles in prose and poetry vs. sentence \nlengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length") + theme(text = element_text(size = 18))
##########CLAUSE DETAIL####################
iota.table <- read.csv("C:/Users/T470s/Documents/GitHub/ma_thesis_23-24/Data-output/var-info/all-clause-3.16.24.csv", stringsAsFactors = TRUE, comment.char = "#")
View(iota.table)
iota.table[iota.table$WORK %in% poetry & iota.table$SUB == "quoniam",]
iota.table[iota.table$WORK %in% poetry & iota.table$SUB == "quoniam_causal",]
View(iota.table[iota.table$WORK %in% poetry & iota.table$SUB == "quoniam_causal",])
View(mu.table)
View(mu.table[mu.table$WORK == "Pere" & mu.table$FULL.PARENTH != "id est",])
#choice of causal (p. 57-58 of the master's thesis, "most common lemmas" subheading); WAIT, THE TEST SHOULD BE ABOUT POETRY VS. PROSE, SEE BELOW
causal.table <- droplevels(eta.table[eta.table$LEM %in% c("quod", "quia", "quoniam"),])
causal.table$GENRE.B <- causal.table$WORK %in% colloquial.nosat
causal.table$LEM <- droplevels(causal.table$LEM)
causal.conting <- table(causal.table$GENRE.B, causal.table$LEM)
causal.conting
chisq.causal <- chisq.test(causal.conting)
chisq.causal
#choice of causal (p. 57-58 of the master's thesis, "most common lemmas" subheading); WAIT, THE TEST SHOULD BE ABOUT POETRY VS. PROSE, SEE BELOW
causal.table <- droplevels(eta.table[eta.table$LEM %in% c("quod", "quia", "quoniam"),])
causal.table$GENRE.B <- causal.table$WORK %in% colloquial.nosat & causal.table$WORK != "Pere"
causal.table$LEM <- droplevels(causal.table$LEM)
causal.conting <- table(causal.table$GENRE.B, causal.table$LEM)
causal.conting
chisq.causal <- chisq.test(causal.conting)
chisq.causal
#choice of causal (p. 57-58 of the master's thesis, "most common lemmas" subheading); WAIT, THE TEST SHOULD BE ABOUT POETRY VS. PROSE, SEE BELOW
causal.table <- droplevels(eta.table[eta.table$LEM %in% c("quod", "quoniam"),])
causal.table$GENRE.B <- causal.table$WORK %in% colloquial.nosat & causal.table$WORK != "Pere"
causal.table$LEM <- droplevels(causal.table$LEM)
causal.conting <- table(causal.table$GENRE.B, causal.table$LEM)
causal.conting
chisq.causal <- chisq.test(causal.conting)
chisq.causal
#choice of causal (p. 57-58 of the master's thesis, "most common lemmas" subheading); WAIT, THE TEST SHOULD BE ABOUT POETRY VS. PROSE, SEE BELOW
causal.table <- droplevels(eta.table[eta.table$LEM %in% c("quia", "quoniam"),])
causal.table$GENRE.B <- causal.table$WORK %in% colloquial.nosat & causal.table$WORK
causal.table$LEM <- droplevels(causal.table$LEM)
causal.conting <- table(causal.table$GENRE.B, causal.table$LEM)
causal.conting
chisq.causal <- chisq.test(causal.conting)
chisq.causal
#choice of causal (p. 57-58 of the master's thesis, "most common lemmas" subheading); WAIT, THE TEST SHOULD BE ABOUT POETRY VS. PROSE, SEE BELOW
causal.table <- droplevels(eta.table[eta.table$LEM %in% c("quia", "quoniam"),])
causal.table$GENRE.B <- causal.table$WORK %in% colloquial.nosat & causal.table$WORK != "Pere"
causal.table$LEM <- droplevels(causal.table$LEM)
causal.conting <- table(causal.table$GENRE.B, causal.table$LEM)
causal.conting
chisq.causal <- chisq.test(causal.conting)
chisq.causal
#choice of causal (p. 57-58 of the master's thesis, "most common lemmas" subheading); WAIT, THE TEST SHOULD BE ABOUT POETRY VS. PROSE, SEE BELOW
causal.table <- droplevels(eta.table[eta.table$LEM %in% c("quod", "quoniam"),])
causal.table$GENRE.B <- causal.table$WORK %in% colloquial.nosat & causal.table$WORK != "Pere"
causal.table$LEM <- droplevels(causal.table$LEM)
causal.conting <- table(causal.table$GENRE.B, causal.table$LEM)
causal.conting
chisq.causal <- chisq.test(causal.conting)
chisq.causal
causal.table <- droplevels(eta.table[eta.table$LEM %in% c("quod", "quia", "quoniam") & eta.table$WORK %in% colloquial.nosat,])
table(causal.table$WORK, causal.table$LEM)
causal.table <- droplevels(iota.table[iota.table$SUB %in% c("quod_causal", "quia_causal", "quoniam_causal") & eta.table$WORK %in% colloquial.nosat,])
causal.table <- droplevels(iota.table[iota.table$SUB %in% c("quod_causal", "quia_causal", "quoniam_causal") & iota.table$WORK %in% colloquial.nosat,])
table(causal.table$WORK, causal.table$LEM)
table(causal.table$WORK, causal.table$SUB)
causal.table <- droplevels(iota.table[iota.table$SUB %in% c("quod_causal", "quia_causal", "quoniam_causal"),])
table(causal.table$WORK, causal.table$SUB)
#participial phrases in non-colloquial texts \nvs. sentence lengths fig 3-1 2
cortests <- c(prose.noncoll, poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 60)) + ylim(c(0,100)) + labs(title=paste("Participial phrases in non-colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length") + theme(text = element_text(size = 18));png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-1 2.png")
#using ggplot2 library for correlation
library(ggplot2)
#store the divisions of the text into variables. Each string in the vectors is a short title used in the WORK column of nu.table. colloquial.nosat is an artifact of the removal of the satires.
colloquial.nosat <- c("Pere", "Petr Speech", "Att", "Fab")
#participial phrases in non-colloquial texts \nvs. sentence lengths fig 3-1 2
cortests <- c(prose.noncoll, poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 60)) + ylim(c(0,100)) + labs(title=paste("Participial phrases in non-colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length") + theme(text = element_text(size = 18));png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-1 2.png")
#prepositions in prose (colloquial and non-colloquial) \nvs. sentence lengths fig 3-2 1
cortests <- c(prose.noncoll); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PREPOSITION[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PREPOSITION, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 30)) + ylim(c(0,100)) + labs(title=paste("Prepositional phrases in prose \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of prepositional phrases") + ylab("Sentence length") + theme(text = element_text(size = 18)); png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 1.png")
#prepositional phrases in poetry \nvs. sentence lengths fig 3-2 2
cortests <- c(poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PREPOSITION[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PREPOSITION, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 10)) + ylim(c(0,100)) + labs(title=paste("Prepositional phrases in poetry \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of prepositional phrases") + ylab("Sentence length") + theme(text = element_text(size = 18)); png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 2.png")
#prepositional phrases in colloquial texts \nvs. sentence lengths fig 3-2 3
cortests <- c(colloquial.nosat); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PREPOSITION[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PREPOSITION, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 10)) + ylim(c(0,100)) + labs(title=paste("Prepositional phrases in colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of prepositional phrases") + ylab("Sentence length") + theme(text = element_text(size = 18)); png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 3.png")
#Pronouns in colloquial texts \nvs. sentence lengths fig 3-3 1
cortests <- c(colloquial.nosat); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PRONOUN[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PRONOUN, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Pronouns in colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of pronouns") + ylab("Sentence length") + theme(text = element_text(size = 18)); png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 1.png")
#Pronouns in poetry \nvs. sentence lengths
cortests <- c(poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PRONOUN[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PRONOUN, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Pronouns in poetry \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of pronouns") + ylab("Sentence length") + theme(text = element_text(size = 18)); png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 2.png")
#pronouns in prose \nvs. sentence lengths
cortests <- c(prose.noncoll); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PRONOUN[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PRONOUN, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Pronouns in prose \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of pronouns") + ylab("Sentence length") + theme(text = element_text(size = 18)); png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 3.png")
#clear memory
rm(list = ls())
#set working directory to r-code in the GitHub repo
setwd("C:/Users/T470s/Documents/GitHub/ma_thesis_23-24/r-code")
#(pos-summary; name of the data spreadsheet and the xquery file)
#nu-table was a naming convention used during the analysis stage
nu.table <- read.csv("./../Data-output/var-info/pos-summary-3.15.24.csv", stringsAsFactors = TRUE, comment.char = "#", quote = "")
#using ggplot2 library for correlation
library(ggplot2)
#store the divisions of the text into variables. Each string in the vectors is a short title used in the WORK column of nu.table. colloquial.nosat is an artifact of the removal of the satires.
colloquial.nosat <- c("Pere", "Petr Speech", "Att", "Fab")
poetry <-  c("Elegie", "Elegia", "Sati", "Aen", "Met", "Carm", "Amor")
prose.noncoll <- c("(In Cat|Again)", "Hist", "Res", "Aug", "Ann", "Cael", "Gall", "off", "Vul", "agri")
prose <- c("Pere", "(In Cat|Again)", "Hist", "Res", "Aug", "Ann", "Cael", "Gall", "Att", "off", "Vul", "agri")
#FILE EXPORT SETTINGS
width <- 580;
height <- 460;
#colloquial participles fig 3-1 1
cortests <- c("Petr Speech", "Fab", "Att"); set.seed(sum(utf8ToInt("stuffandthings")));
spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman");
ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 60)) + ylim(c(0,100)) + labs(title=paste("Participles in colloquial (no Egeria) texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length") + theme(text = element_text(size = 18));
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-1 1.png")
?png
?dev.off
dev.print(png, file = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-1 1.png")
#participial phrases in non-colloquial texts \nvs. sentence lengths fig 3-1 2
cortests <- c(prose.noncoll, poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 60)) + ylim(c(0,100)) + labs(title=paste("Participial phrases in non-colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length") + theme(text = element_text(size = 18));
dev.off()
#participial phrases in non-colloquial texts \nvs. sentence lengths fig 3-1 2
cortests <- c(prose.noncoll, poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 60)) + ylim(c(0,100)) + labs(title=paste("Participial phrases in non-colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length") + theme(text = element_text(size = 18));
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-1 2.png")
dev.off()
#prepositions in prose (colloquial and non-colloquial) \nvs. sentence lengths fig 3-2 1
cortests <- c(prose.noncoll); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PREPOSITION[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PREPOSITION, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 30)) + ylim(c(0,100)) + labs(title=paste("Prepositional phrases in prose \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of prepositional phrases") + ylab("Sentence length") + theme(text = element_text(size = 18));
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 1.png")
dev.off()
#prepositional phrases in poetry \nvs. sentence lengths fig 3-2 2
cortests <- c(poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PREPOSITION[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PREPOSITION, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 10)) + ylim(c(0,100)) + labs(title=paste("Prepositional phrases in poetry \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of prepositional phrases") + ylab("Sentence length") + theme(text = element_text(size = 18)); png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 2.png")
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 2.png")
dev.off()
#prepositional phrases in colloquial texts \nvs. sentence lengths fig 3-2 3
cortests <- c(colloquial.nosat); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PREPOSITION[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PREPOSITION, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 10)) + ylim(c(0,100)) + labs(title=paste("Prepositional phrases in colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of prepositional phrases") + ylab("Sentence length") + theme(text = element_text(size = 18));
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 3.png")
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 3.png")
dev.off()
#Pronouns in colloquial texts \nvs. sentence lengths fig 3-3 1
cortests <- c(colloquial.nosat); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PRONOUN[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PRONOUN, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Pronouns in colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of pronouns") + ylab("Sentence length") + theme(text = element_text(size = 18));
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 1.png")
dev.off()
#Pronouns in poetry \nvs. sentence lengths
cortests <- c(poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PRONOUN[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PRONOUN, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Pronouns in poetry \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of pronouns") + ylab("Sentence length") + theme(text = element_text(size = 18)); png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 2.png")
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 2.png")
dev.off()
#pronouns in prose \nvs. sentence lengths
cortests <- c(prose.noncoll); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PRONOUN[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PRONOUN, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Pronouns in prose \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of pronouns") + ylab("Sentence length") + theme(text = element_text(size = 18));
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 3.png")
dev.off()
#participial phrases in non-colloquial texts \nvs. sentence lengths fig 3-1 2
cortests <- c(prose.noncoll, poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 60)) + ylim(c(0,100)) + labs(title=paste("Participial phrases in non-colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length") + theme(text = element_text(size = 18));
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-1 2.png")
dev.off()
#participial phrases in non-colloquial texts \nvs. sentence lengths fig 3-1 2
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-1 2.png")
cortests <- c(prose.noncoll, poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 60)) + ylim(c(0,100)) + labs(title=paste("Participial phrases in non-colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length") + theme(text = element_text(size = 18));
dev.off()
#prepositions in prose (colloquial and non-colloquial) \nvs. sentence lengths fig 3-2 1
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 1.png")
cortests <- c(prose.noncoll); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PREPOSITION[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PREPOSITION, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 30)) + ylim(c(0,100)) + labs(title=paste("Prepositional phrases in prose \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of prepositional phrases") + ylab("Sentence length") + theme(text = element_text(size = 18));
dev.off()
#prepositional phrases in poetry \nvs. sentence lengths fig 3-2 2
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 2.png")
cortests <- c(poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PREPOSITION[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PREPOSITION, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 10)) + ylim(c(0,100)) + labs(title=paste("Prepositional phrases in poetry \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of prepositional phrases") + ylab("Sentence length") + theme(text = element_text(size = 18));
dev.off()
#prepositional phrases in colloquial texts \nvs. sentence lengths fig 3-2 3
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-2 3.png")
cortests <- c(colloquial.nosat); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PREPOSITION[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PREPOSITION, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 10)) + ylim(c(0,100)) + labs(title=paste("Prepositional phrases in colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of prepositional phrases") + ylab("Sentence length") + theme(text = element_text(size = 18));
dev.off()
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 1.png")
cortests <- c(colloquial.nosat); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PRONOUN[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PRONOUN, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Pronouns in colloquial texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of pronouns") + ylab("Sentence length") + theme(text = element_text(size = 18));
dev.off()
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 2.png")
cortests <- c(poetry); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PRONOUN[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PRONOUN, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Pronouns in poetry \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of pronouns") + ylab("Sentence length") + theme(text = element_text(size = 18));
dev.off()
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-3 3.png")
cortests <- c(prose.noncoll); set.seed(sum(utf8ToInt("stuffandthings"))); spearman <- cor.test(x=jitter(nu.table$PRONOUN[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PRONOUN, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 15)) + ylim(c(0,100)) + labs(title=paste("Pronouns in prose \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of pronouns") + ylab("Sentence length") + theme(text = element_text(size = 18));
dev.off()
png(filename = "C:/Users/T470s/Documents/2024-Spring/Thesis/Ch. 3 edits/Correlation edits/fig 3-1 1.png")
cortests <- c("Petr Speech", "Fab", "Att"); set.seed(sum(utf8ToInt("stuffandthings")));spearman <- cor.test(x=jitter(nu.table$PARTICIPLE[nu.table$WORK %in% cortests]), y=jitter(nu.table$SENTLEN[nu.table$WORK %in% cortests]), method="spearman"); ggplot(nu.table[nu.table$WORK %in% cortests,], aes(x=jitter(PARTICIPLE, factor = 2), y=jitter(SENTLEN))) + geom_point() + xlim(c(0, 60)) + ylim(c(0,100)) + labs(title=paste("Participles in colloquial (no Egeria) texts \nvs. sentence lengths. Spearman:", as.character(round(spearman$estimate, 2)))) + geom_smooth(method="lm") + xlab("Count of participles") + ylab("Sentence length") + theme(text = element_text(size = 18));
dev.off()
